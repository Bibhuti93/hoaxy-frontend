{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to import log:  botornot.log201702\n",
      "Finished importing log:  botornot.log201702\n",
      "87.63839745521545 seconds elapsed\n",
      "Feedback log Import Process Completed!\n",
      "LOG IMPORT PROCESS INFORMATION:\n",
      "total-lines-parsed:  3953169\n",
      "records-committed:  23\n",
      "non-json-lines:  40370\n",
      "non-flag-json-type:  3912775\n",
      "json-with-no-type:  0\n",
      "non-proper-fields-upon-retrieval:  1\n",
      "db-commit-failures:  0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "#Author: Mihai Avram, e-mail: mihai.v.avram@gmail.com\n",
    "\n",
    "#TODO BEFORE RUNNING: Change database configuration settings in pgsqlconn and also log_path and change the log_file_list depending on the logs one wants to import\n",
    "\n",
    "#ALL IMPORTS\n",
    "#for parsing the data in the logs\n",
    "import json\n",
    "#for connecting to the database\n",
    "import psycopg2\n",
    "#for error logging\n",
    "import sys, traceback\n",
    "#for timing purposes\n",
    "import time\n",
    "\n",
    "#can be used for debugging if exceptions start to occur\n",
    "def print_exception():\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    traceback.print_exception(exc_type, exc_value, exc_traceback, limit=3, file=sys.stdout)\n",
    "\n",
    "#ALL FUNCTIONS\n",
    "\n",
    "#inserts feedback log to database\n",
    "def feedback_insertion_script(source_user_id, target_user_id, target_screen_name, time_stamp, feedback_label, \\\n",
    "                              feedback_text, target_profile, target_timeline_tweets, target_mention_tweets):\n",
    "    \n",
    "    botbase_cursor.execute(\"\"\"INSERT INTO public.feedback(source_user_id, target_user_id, target_screen_name, time_stamp, feedback_label,\n",
    "                        feedback_text, target_profile, target_timeline_tweets, target_mention_tweets) \n",
    "                              VALUES \n",
    "                (%s, %s, %s, to_timestamp(%s), %s, %s, %s, %s, %s);\"\"\", \\\n",
    "                (source_user_id, target_user_id, target_screen_name, time_stamp, feedback_label, \\\n",
    "                feedback_text, target_profile, target_timeline_tweets, target_mention_tweets))\n",
    "\n",
    "    #commiting changes\n",
    "    pgsqlconn.commit()\n",
    "\n",
    "#GLOBAL VARIABLES\n",
    "total_number_of_lines_parsed = 0\n",
    "records_committed = 0\n",
    "errors_and_informational_count = 0\n",
    "unmatched_botscore_category_schema_count = 0\n",
    "json_not_proper_log_count = 0\n",
    "json_with_no_type_count = 0\n",
    "failed_to_retrieve_proper_fields_count = 0\n",
    "failed_to_commit_to_db_count = 0\n",
    "    \n",
    "#MAIN CODE\n",
    "if __name__ == '__main__':\n",
    "    #connecting to the database\n",
    "    pgsqlconn = psycopg2.connect(host='', user='', password='', dbname='', port='')\n",
    "    #cursor needed to execute db operations\n",
    "    botbase_cursor = pgsqlconn.cursor()\n",
    "    #starting timer\n",
    "    timer_start = time.time()\n",
    "    \n",
    "    #log name and location information\n",
    "    log_path = ''\n",
    "    \n",
    "    log_file_list = ['botornot.log201506', 'botornot.log201510', 'botornot.log201605', 'botornot.log201701' ,'botornot.log201702', 'botornot.log201705', 'botornot.log.2017-05-14', 'botornot.log.2017-05-21', 'botornot.log.2017-05-28', 'botornot.log.2017-06-04', 'botornot.log.2017-06-11', 'botornot.log.2017-06-18', 'botornot.log.2017-06-25', 'botornot.log.2017-07-02', 'botornot.log.2017-07-09', 'botornot.log.2017-07-16','botornot.log.2017-07-23', 'botornot.log.2017-07-30', 'botornot.log.2017-08-06', 'botornot.log.2017-08-13']\n",
    "  \n",
    "    #log to store any errors due to the logs not containing the proper data (i.e. other logging information such as errors or other requests)\n",
    "    error_log_file = open(\"feedbackinsertionlog.err\", \"a\")\n",
    "  \n",
    "    #iterating through all log files\n",
    "    for log in log_file_list:\n",
    "        print(\"Starting to import log: \", log)\n",
    "        sys.stdout.flush()\n",
    "        file_location = log_path + log\n",
    "\n",
    "        #parsing logs and uploading the entries to the botometer database\n",
    "        log_file = open(file_location,\"r\")\n",
    "\n",
    "        for line_num, line in enumerate(log_file, start = 1):\n",
    "            total_number_of_lines_parsed = total_number_of_lines_parsed + 1\n",
    "            \n",
    "            #checking if the current line is json, if not then this line should not be parsed because we are only looking for json log lines\n",
    "            try: \n",
    "                line_json = json.loads(line)\n",
    "            except:\n",
    "                errors_and_informational_count = errors_and_informational_count + 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if not line_json[\"type\"] == \"flag\":\n",
    "                    json_not_proper_log_count = json_not_proper_log_count + 1\n",
    "                    continue\n",
    "            except:\n",
    "                json_with_no_type_count = json_with_no_type_count + 1\n",
    "                error_log_file.write(\"NO-LOG-TYPE-JSON INFO---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                continue\n",
    "                \n",
    "            #parsing json line and retrieving the proper fields regarding the user i.e. user id, screen name, tweets, etc...\n",
    "            try:\n",
    "                source_user_id = None\n",
    "                target_user_id = line_json[\"user\"][\"id\"]\n",
    "                target_screen_name = str(line_json[\"user\"][\"screen_name\"])\n",
    "                #changing @screenname to screenname to add to the database if found\n",
    "                if target_screen_name[0:1] == \"@\":\n",
    "                    target_screen_name = target_screen_name[1:]\n",
    "                if len(target_screen_name) > 15:\n",
    "                    #user may have a screen-name logged as longer than 15 characters which is not proper in Twitter and could be instead the userid or some other error so we make it none\n",
    "                    target_screen_name = None\n",
    "                time_stamp = line_json[\"timestamp\"]\n",
    "                #some timestamps are stored in milliseconds so for those we divide by 1000\n",
    "                if len(str(time_stamp)) >= 12:\n",
    "                    time_stamp = time_stamp/1000\n",
    "                feedback_label = None\n",
    "                feedback_text = line_json['text']\n",
    "                target_profile = None\n",
    "                target_timeline_tweets = None\n",
    "                target_mention_tweets = None\n",
    "            except:\n",
    "                error_log_file.write(\"NON-PROPER-FIELDS ERROR---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                failed_to_retrieve_proper_fields_count = failed_to_retrieve_proper_fields_count + 1\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                #inserting data to the database\n",
    "                feedback_insertion_script(source_user_id, target_user_id, target_screen_name, time_stamp, feedback_label, \\\n",
    "                    feedback_text, target_profile, target_timeline_tweets, target_mention_tweets)\n",
    "                records_committed = records_committed + 1\n",
    "            except:\n",
    "                error_log_file.write(\"DB INSERTION ERROR---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                failed_to_commit_to_db_count = failed_to_commit_to_db_count + 1  \n",
    "                continue\n",
    "\n",
    "        print(\"Finished importing log: \", log)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    #closing access to database\n",
    "    botbase_cursor.close()\n",
    "    pgsqlconn.close()\n",
    "\n",
    "    #closing log files\n",
    "    log_file.close()\n",
    "    error_log_file.close()\n",
    "\n",
    "    #ending and evaluating time elapsed\n",
    "    print(\"%s seconds elapsed\" % (time.time()-timer_start))\n",
    "    print(\"Feedback log Import Process Completed!\")\n",
    "    \n",
    "    #printing log statistics\n",
    "    print(\"LOG IMPORT PROCESS INFORMATION:\")\n",
    "    print(\"total-lines-parsed: \", total_number_of_lines_parsed)\n",
    "    print(\"records-committed: \", records_committed)\n",
    "    print(\"non-json-lines: \",errors_and_informational_count)\n",
    "    print(\"non-flag-json-type: \", json_not_proper_log_count)\n",
    "    print(\"json-with-no-type: \", json_with_no_type_count)\n",
    "    print(\"non-proper-fields-upon-retrieval: \", failed_to_retrieve_proper_fields_count)\n",
    "    print(\"db-commit-failures: \", failed_to_commit_to_db_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
