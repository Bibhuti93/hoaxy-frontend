{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to import log:  test\n",
      "Finished importing log:  test\n",
      "0.34769105911254883 seconds elapsed\n",
      "Log Import Process Completed!\n",
      "LOG IMPORT PROCESS INFORMATION:\n",
      "total-lines-parsed:  1\n",
      "records-committed:  1\n",
      "non-json-lines:  0\n",
      "non-log-json-type:  0\n",
      "json-with-no-type:  0\n",
      "non-matched-proper-score-category-schema:  0\n",
      "non-proper-fields-upon-retrieval:  0\n",
      "db-commit-failures:  0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/evn python\n",
    "\n",
    "#Author: Mihai Avram, e-mail: mihai.v.avram@gmail.com\n",
    "\n",
    "#ALL IMPORTS\n",
    "#for parsing the data in the logs\n",
    "import json\n",
    "#for connecting to the database\n",
    "import psycopg2\n",
    "#for error logging\n",
    "import sys, traceback\n",
    "#for timing purposes\n",
    "import time\n",
    "\n",
    "#ALL FUNCTIONS\n",
    "#function for deciding on a score value to use for bot_score_english and bot_score_universal depending on what's available in the log\n",
    "def score_decider(potential_score_keys, line_json):\n",
    "    for key in potential_score_keys:\n",
    "        if len(key) == 1:\n",
    "            key1 = key[0]\n",
    "            try:\n",
    "                score = line_json[key1]\n",
    "                return score\n",
    "            except:\n",
    "                continue\n",
    "        elif len(key) == 2:\n",
    "            key1 = key[0]\n",
    "            key2 = key[1]\n",
    "            try:\n",
    "                score = line_json[key1][key2]\n",
    "                return score\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "#inserts log to database\n",
    "def log_insertion_script(user_id, screen_name, time_stamp, all_bot_scores, bot_score_english, \\\n",
    "                bot_score_universal, requester_ip, tweets_per_day, num_submitted_timeline_tweets, \\\n",
    "                num_submitted_mention_tweets, num_requests):\n",
    "    \n",
    "    botbase_cursor.execute(\"\"\"INSERT INTO public.botscore(\n",
    "                user_id, screen_name, time_stamp, all_bot_scores, bot_score_english, \n",
    "                bot_score_universal, requester_ip, tweets_per_day, num_submitted_timeline_tweets, \n",
    "                num_submitted_mention_tweets, num_requests) \n",
    "                              VALUES \n",
    "                (%s, %s, to_timestamp(%s), %s, %s, %s, %s, %s, %s, %s, %s);\"\"\", \\\n",
    "                (user_id, screen_name, time_stamp, json.dumps(all_bot_scores), bot_score_english, \\\n",
    "                bot_score_universal, requester_ip, tweets_per_day, num_submitted_timeline_tweets, \\\n",
    "                num_submitted_mention_tweets, num_requests))\n",
    "\n",
    "    #commiting changes\n",
    "    pgsqlconn.commit()\n",
    "\n",
    "#GLOBAL VARIABLES\n",
    "total_number_of_lines_parsed = 0\n",
    "records_committed = 0\n",
    "errors_and_informational_count = 0\n",
    "unmatched_botscore_category_schema_count = 0\n",
    "json_not_proper_log_count = 0\n",
    "json_with_no_type_count = 0\n",
    "failed_to_retrieve_proper_fields_count = 0\n",
    "failed_to_commit_to_db_count = 0\n",
    "    \n",
    "#MAIN CODE\n",
    "if __name__ == '__main__':\n",
    "    #connecting to the database\n",
    "    pgsqlconn = psycopg2.connect(host='localhost', user='postgres', password='password', dbname='botbase')\n",
    "    #cursor needed to execute db operations\n",
    "    botbase_cursor = pgsqlconn.cursor()\n",
    "    #starting timer\n",
    "    timer_start = time.time()\n",
    "    \n",
    "    #log name and location information\n",
    "    #log_path = '/home/mavram/Research/HoaxyBotometer/ImportBackuplogsTask/logs/backups/unzipstage/'\n",
    "    log_path = '/media/marvram/OS/Research/HoaxyBotometer/ImportBackuplogsTask/logs/'\n",
    "    log_file_list = ['botornot.log.2017-09-03','botornot.log.2017-09-10']\n",
    "                    #'botornot.log201506',\n",
    "                     #, 'botornot.log201510', 'botornot.log201605', 'botornot.log201701', \\\n",
    "                     #'botornot.log201702', 'botornot.log201705', 'botornot.log.2017-05-14', 'botornot.log.2017-05-21', \\\n",
    "                     #'botornot.log.2017-05-28', 'botornot.log.2017-06-04', 'botornot.log.2017-06-11', 'botornot.log.2017-06-18', \\\n",
    "                     #'botornot.log.2017-06-25', 'botornot.log.2017-07-02', 'botornot.log.2017-07-09', 'botornot.log.2017-07-16', \\\n",
    "                     #'botornot.log.2017-07-23', 'botornot.log.2017-07-30', 'botornot.log.2017-08-06', 'botornot.log.2017-08-13']\n",
    "                    #recent\n",
    "                    #botornot.log.2017-08-20  botornot.log.2017-08-27\n",
    "    #log to store any errors due to the logs not containing the proper data (i.e. other logging information such as errors or other requests)\n",
    "    error_log_file = open(\"botscoreloginsertion.err\", \"a\")\n",
    "  \n",
    "    #iterating through all log files\n",
    "    for log in log_file_list:\n",
    "        print(\"Starting to import log: \", log)\n",
    "        sys.stdout.flush()\n",
    "        file_location = log_path + log\n",
    "\n",
    "        #parsing logs and uploading the entries to the botometer database\n",
    "        log_file = open(file_location,\"r\")\n",
    "\n",
    "        for line_num, line in enumerate(log_file, start = 1):\n",
    "            total_number_of_lines_parsed = total_number_of_lines_parsed + 1\n",
    "            \n",
    "            #checking if the current line is json, if not then this line should not be parsed because we are only looking for json log lines\n",
    "            try: \n",
    "                line_json = json.loads(line)\n",
    "            except:\n",
    "                errors_and_informational_count = errors_and_informational_count + 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if not line_json[\"type\"] == \"log\":\n",
    "                    json_not_proper_log_count = json_not_proper_log_count + 1\n",
    "                    continue\n",
    "            except:\n",
    "                json_with_no_type_count = json_with_no_type_count + 1\n",
    "                error_log_file.write(\"NO-LOG-TYPE-JSON INFO---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            #parsing json line and retrieving the proper fields regarding the user i.e. user id, screen name, tweets, etc...\n",
    "            try:\n",
    "                user_id = line_json[\"search\"][\"user_id\"]\n",
    "                screen_name = str(line_json[\"search\"][\"sn\"])\n",
    "                if len(screen_name) > 15:\n",
    "                    #user may have a screen-name logged as longer than 15 characters which is not proper in Twitter and could be instead the userid or some other error so we make it none\n",
    "                    screen_name = None\n",
    "                time_stamp = line_json[\"timestamp\"]\n",
    "                #some timestamps are stored in milliseconds so for those we divide by 1000\n",
    "                if len(str(time_stamp)) >= 12:\n",
    "                    time_stamp = time_stamp/1000\n",
    "                botscore_representation = line_json[\"categories\"]\n",
    "                \n",
    "                #parsing the bot scores to match the schema from here https://market.mashape.com/OSoMe/botometer\n",
    "                try:\n",
    "                    friend_score = round(botscore_representation['friend_classification'], 2)\n",
    "                    sentiment_score = round(botscore_representation['sentiment_classification'], 2)\n",
    "                    temporal_score = round(botscore_representation['temporal_classification'], 2)\n",
    "                    user_score = round(botscore_representation['user_classification'], 2)\n",
    "                    network_score = round(botscore_representation['network_classification'], 2)\n",
    "                    content_score = round(botscore_representation['content_classification'], 2)\n",
    "                    all_bot_scores = {\"friend\": friend_score, \"sentiment\": sentiment_score, \"temporal\": temporal_score, \"user\": user_score, \"network\": network_score, \"content\": content_score}\n",
    "                except:\n",
    "                    #parsing another representation which is exactly as the mashape botscore api\n",
    "                    try:\n",
    "                        friend_score = round(botscore_representation['friend'], 2)\n",
    "                        sentiment_score = round(botscore_representation['sentiment'], 2)\n",
    "                        temporal_score = round(botscore_representation['temporal'], 2)\n",
    "                        user_score = round(botscore_representation['user'], 2)\n",
    "                        network_score = round(botscore_representation['network'], 2)\n",
    "                        content_score = round(botscore_representation['content'], 2)\n",
    "                        all_bot_scores = {\"friend\": friend_score, \"sentiment\": sentiment_score, \"temporal\": temporal_score, \"user\": user_score, \"network\": network_score, \"content\": content_score}\n",
    "                    except:\n",
    "                        #parsing a list schema instead of json schema i.e. [[\"network\",0.5], [\"sentiment\",0.2], ...]\n",
    "                        try:\n",
    "                            friend_score = round(botscore_representation[5][1], 2)\n",
    "                            sentiment_score = round(botscore_representation[1][1], 2)\n",
    "                            temporal_score = round(botscore_representation[2][1], 2)\n",
    "                            user_score = round(botscore_representation[4][1], 2)\n",
    "                            network_score = round(botscore_representation[0][1], 2)\n",
    "                            content_score = round(botscore_representation[3][1], 2)\n",
    "                            all_bot_scores = {\"friend\": friend_score, \"sentiment\": sentiment_score, \"temporal\": temporal_score, \"user\": user_score, \"network\": network_score, \"content\": content_score}                            \n",
    "                        except:\n",
    "                             #score schema does not include the needed scores so will insert as null\n",
    "                            unmatched_botscore_category_schema_count = unmatched_botscore_category_schema_count + 1\n",
    "                            error_log_file.write(\"NON-MATCHED-CATEGORY-SCHEMA INFO---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                            all_bot_scores = None\n",
    "                #english bot score which is either found in line_json[\"score\"], line_json[\"classification\"], line_json[\"score\"][\"english\"]\n",
    "                keys = [[\"score\",\"english\"],[\"score\"],[\"classification\"]]\n",
    "                bot_score_english = score_decider(keys, line_json)\n",
    "                #universal bot score which is either found in line_json[\"score\"][\"universal\"] or line_json[\"categories\"][\"languageagnostic_classification\"] otherwise null\n",
    "                keys = [[\"score\",\"universal\"],[\"categories\",\"languageagnostic_classification\"]]\n",
    "                bot_score_universal = score_decider(keys, line_json)\n",
    "                #storing a comma delimited string of ips\n",
    "                requester_ip = line_json[\"remote_ip\"]\n",
    "                #some ips are stored in lists and some not, must distinguish and treat them separately here\n",
    "                #in order to yield <ip1>,<ip2>,etc...\n",
    "                if type(requester_ip) == list:\n",
    "                    requester_ip = ','.join(line_json[\"remote_ip\"])                \n",
    "                tweets_per_day = None\n",
    "                num_submitted_timeline_tweets = None\n",
    "                num_submitted_mention_tweets = None\n",
    "                num_requests = 0\n",
    "            except:\n",
    "                error_log_file.write(\"NON-PROPER-FIELDS ERROR---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                failed_to_retrieve_proper_fields_count = failed_to_retrieve_proper_fields_count + 1\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                #inserting data to the database\n",
    "                log_insertion_script(user_id, screen_name, time_stamp, all_bot_scores, bot_score_english, bot_score_universal, \\\n",
    "                            str(requester_ip), tweets_per_day, num_submitted_timeline_tweets, num_submitted_mention_tweets, num_requests)\n",
    "                records_committed = records_committed + 1\n",
    "            except:\n",
    "                error_log_file.write(\"DB INSERTION ERROR---File: \" + log + \" LineNumber: \" + str(line_num) + \" Error: \" + str(sys.exc_info()[0]) + \"\\n\")\n",
    "                failed_to_commit_to_db_count = failed_to_commit_to_db_count + 1          \n",
    "                continue\n",
    "\n",
    "        print(\"Finished importing log: \", log)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    #closing access to database\n",
    "    botbase_cursor.close()\n",
    "    pgsqlconn.close()\n",
    "\n",
    "    #closing log files\n",
    "    log_file.close()\n",
    "    error_log_file.close()\n",
    "\n",
    "    #ending and evaluating time elapsed\n",
    "    print(\"%s seconds elapsed\" % (time.time()-timer_start))\n",
    "    print(\"Log Import Process Completed!\")\n",
    "    \n",
    "    #printing log statistics\n",
    "    print(\"LOG IMPORT PROCESS INFORMATION:\")\n",
    "    print(\"total-lines-parsed: \", total_number_of_lines_parsed)\n",
    "    print(\"records-committed: \", records_committed)\n",
    "    print(\"non-json-lines: \",errors_and_informational_count)\n",
    "    print(\"non-log-json-type: \", json_not_proper_log_count)\n",
    "    print(\"json-with-no-type: \", json_with_no_type_count)\n",
    "    print(\"non-matched-proper-score-category-schema: \", unmatched_botscore_category_schema_count)\n",
    "    print(\"non-proper-fields-upon-retrieval: \", failed_to_retrieve_proper_fields_count)\n",
    "    print(\"db-commit-failures: \", failed_to_commit_to_db_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
